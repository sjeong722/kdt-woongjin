import pendulum
from airflow import DAG
from airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator
"""
ðŸ”¸ SQLExecuteQueryOperator
    - íŠ¹ì • ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ëŒ€ìƒìœ¼ë¡œ SQL ì¿¼ë¦¬ë¥¼ ì‹¤í–‰í•  ìˆ˜ ìžˆë„ë¡ í•´ì£¼ëŠ” ì˜¤í¼ë ˆì´í„°
    - Airflow UIì—ì„œ ëŒ€ìƒ DBì— ëŒ€í•œ connection ì„¤ì • í•„ìš”!

ðŸ”¸ Connection ì„¤ì • ì •ë³´
    - Conn Id : supabase_conn
    - Host : AWS / Supabase ë“± ì™¸ë¶€ DB Host
    - Schema : postgres

https://airflow.apache.org/docs/apache-airflow-providers-common-sql/stable/_api/airflow/providers/common/sql/operators/sql/index.html#airflow.providers.common.sql.operators.sql.SQLExecuteQueryOperator
"""

default_args = dict(
    owner = 'popcorn',
    email = ['datapopcorn@gmail.com'],
    email_on_failure = False,
    retries = 3
    )

with DAG(
    dag_id="08_database_operator_dag",
    start_date=pendulum.datetime(2025, 8, 1, tz='Asia/Seoul'),
    schedule="30 10 * * *",
    default_args = default_args,
    catchup=False
):
    
    DB_CONNECTION_ID = 'supabase_conn'
    
    create_table = SQLExecuteQueryOperator(
        task_id='create_table',
        conn_id=DB_CONNECTION_ID,
        sql="CREATE TABLE IF NOT EXISTS mytable(id INT, name VARCHAR(10));",
    )
    
    insert_rows = SQLExecuteQueryOperator(
        task_id = "insert_rows",
        conn_id = DB_CONNECTION_ID,
        sql = "INSERT INTO mytable VALUES(1,'Ryan'),(2,'Alice'),(3,'Tom');",
    )

    update_rows = SQLExecuteQueryOperator(
        task_id = "update_rows",
        conn_id = DB_CONNECTION_ID,
        sql = "UPDATE mytable SET NAME='Peter' WHERE id=3;",
    )

    # delete_rows = SQLExecuteQueryOperator(
    #     task_id = "delete_rows",
    #     conn_id = DB_CONNECTION_ID,
    #     sql = "DELETE FROM mytable WHERE id=1",
    #     database = 'airflow',
    #     autocommit=True
    # )


create_table >> insert_rows >> update_rows