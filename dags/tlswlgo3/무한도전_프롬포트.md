[캡스톤 프로젝트] 무한도전 레전드 아카이브: 데이터로 부활하는 무도 유니버스

1. 프로젝트 기본 정보
1) 프로젝트명: 유튜브 데이터를 활용한 '무한도전' 에피소드별 인기도 분석 및 방영 정보 아카이빙
2) 팀원: 김승현, 신지해, 윤화영 (6조)
3) 진행 기간: 2026.01.14(수) ~ 2026.01.16(금) (3일)

2. 문제 정의 (Why)
1) 해결하고자 하는 문제: 2005년 시작된 '무한도전'은 종영 8년 차인 현재까지도 '무도 유니버스'라 불리며 마케팅 치트키로 활용되고 있지만, 특정 에피소드의 방영일이나 현재 시점의 인기도를 한눈에 파악할 수 있는 통합 인덱스 부재

2) 왜 이 문제가 중요한가?:
1. 마케팅 활용: 특정 멤버나 시리즈의 현재 화제성을 파악해 광고/협업의 근거로 활용 가능.
2. 정보 제공: '무도짤'은 알지만 상세 정보는 모르는 사용자들에게 정확한 에피소드 정보를 제공하는 '무도 대백과사전' 역할.
3. 타겟 사용자: 예능 콘텐츠 마케터, 무한도전 골수 팬 및 입문자.

3) 핵심 질문 (3개):
[인기 시리즈] 현재 유튜브에서 가장 높은 조회수를 기록하는 무한도전 TOP 3 레전드 시리즈는 무엇인가?
[정보 아카이빙] 영상 제목 속 날짜와 시리즈명을 결합해 '무도 에피소드 타임라인 표'를 구축할 수 있는가?
[멤버 랭킹] 영상 제목에 가장 자주 이름을 올린 '최다 언급 멤버'는 누구인가? (언급 횟수를 기준으로 한 멤버별 지분율 랭킹)

3. 데이터 소스
1) 데이터 출처: YouTube Data API v3 (Search 결과 기반)
2) 수집 주기: 수동 트리거(Manual Trigger) 기반 수시 업데이트 (Airflow 활용)
3) 데이터 볼륨: 약 10,000개 이상의 검색 결과 (인기순/최신순 조합)
4) API/파일 형식: JSON → Python(Antigravity 파싱) → Supabase DB

4. 아키텍처 설계
[YouTube API] → [Airflow] (수집 자동화) → [Google Antigravity] (데이터 정제: 제목에서 날짜/멤버/시리즈명 추출, 조회수 추출) → [Supabase DB] (적재) → [Redash] (시각화)

상세 설계:
1. Airflow DAG 스케줄: Schedule: None (Manual Trigger / 개발 단계 수동 실행)
Tasks: Extract (API 수집) → Transform (Regex 정제) → Load (DB 적재)

2. 데이터 적재 테이블 구조 (infinite_challenge_archive):
video_id (PK), title, view_count, series_name (시리즈명), broadcast_date (방영일), mentioned_members (멤버 리스트)

3. 분석 쿼리 로직:
1) 인기 시리즈: series_name별 view_count 합산 기준 TOP 3 추출
2) 타임라인: broadcast_date 기준 내림차순 정렬 및 에피소드 매핑
3) 멤버 랭킹: mentioned_members 내 이름별 언급 빈도수 집계

5. 구현 체크리스트
Day 1 (수): API 연동 및 정보 추출 로직 개발
1) YouTube API 설정 및 Antigravity 개발 환경 구축
2) 핵심 로직: 제목에서 정규표현식으로 방영날짜(6자리), 멤버이름, 시리즈명을 뽑아내는 Python 코드 작성
3) Airflow 기본 DAG 설정 (주기적 수집 구조 잡기)

Day 2 (목): Supabase 적재 및 데이터 클렌징
1) Supabase 테이블 생성 (mentioned_members는 배열 혹은 텍스트 타입)
2) 정제된 데이터를 DB에 전송하고, 중복 데이터(unique video_id) 처리
3) 누락된 정보(날짜, 시리즈명)를 AI 에이전트(Antigravity)를 통해 보정 및 고도화.

위의 프로젝트를 진행하고싶은데 
아래 조건은 무조건 지켜줘
1. 유튜브 api를 이용하여 데이터를 수집할 건데 수집할 데이터는 아래와 같아.
            'channel_id': item['snippet']['channelId'],
            'video_id': item['id'],
            'title': item['snippet']['title'],
            'description': item['snippet']['description'],
            'thumbnail_url': item['snippet']['thumbnails']['high']['url'],
            'view_count': item['statistics'].get('viewCount', '0'),
            'like_count': item['statistics'].get('likeCount', '0'),
            'comment_count': item['statistics'].get('commentCount', '0'),
            'published_at': item['snippet']['publishedAt'],
            'collected_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')

2. 그리고 데이터 수집 주기는 30분마다 수집하고 전에 수집한 데이터는 중복되지 않도록 수집하게 코드를 작성해
3. supabase에 아래 스키마에 해당하도록 테이블 생성하는 코드도 작성해줘
        create_schema_query = "CREATE SCHEMA IF NOT EXISTS tlswlgo3;"
    create_table_query = """
    CREATE TABLE IF NOT EXISTS tlswlgo3.youtube_videos (
        video_id TEXT PRIMARY KEY,
        channel_id TEXT,
        channel_title TEXT,
        description TEXT,
        thumbnail_url TEXT,
        view_count BIGINT,
        like_count BIGINT,
        comment_count BIGINT,
        published_at TIMESTAMP,
        collected_at TIMESTAMP
    ); 

4. 수집된 데이터를 supabase에 적재하는 코드도 작성해줘

5. airflow를 이용할거여서 dags 형식으로 만들어줘
